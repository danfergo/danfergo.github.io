<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Developing autonomous assistants to help with domestic tasks is a vital topic in robotics research. Among these tasks, garment folding is one of them that is still far from being achieved mainly due to the large number of possible configurations that a crumpled piece of clothing may exhibit. Rese...">
    <meta name="keyword" content="danfergo, programming, code">
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel='alternate' type='application/rss+xml' title='RSS' href='/atom.xml'>

    <title>
        
            GarmNet: Improving Global with Local Perception for Robotic Laundry Folding - Danfergo Research
        
    </title>

    <link rel="canonical" href="http://danfergo.github.io/garmnet/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/hux-blog.css">


    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    <!-- Hamburgers css -->
    
<link rel="stylesheet" href="/css/hamburgers.min.css">


    
<link rel="stylesheet" href="/css/carousel.css">


    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet"
          type="text/css">


    <!-- Hux Delete, sad but pending in China -->
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
          rel='stylesheet' type='text/css'>


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 5.3.0"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
           <!-- <button class="hamburger hamburger--spring navbar-toggle" type="button">
              <span class="hamburger-box">
                <span class="hamburger-inner"></span>
              </span>
            </button> -->
            <!--<button type="button" class="navbar-toggle">-->
            <!--<span class="sr-only">Toggle navigation</span>-->
            <!--<span class="icon-bar"></span>-->
            <!--<span class="icon-bar"></span>-->
            <!--<span class="icon-bar"></span>-->
            <!--</button>-->
            
            <a class="navbar-brand" href="/">
                Danfergo Research
            </a>
            
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <!-- <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                    
                    

                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    

                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                    

                </ul>
            </div> -->
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<!--<script>-->
<!--    // Drop Bootstarp low-performance Navbar-->
<!--    // Use customize navbar with high-quality material design animation-->
<!--    // in high-perf jank-free CSS3 implementation-->
<!--    var $body = document.body;-->
<!--    var $toggle = document.querySelector('.navbar-toggle');-->
<!--    var $navbar = document.querySelector('#huxblog_navbar');-->
<!--    var $collapse = document.querySelector('.navbar-collapse');-->

<!--    $toggle.addEventListener('click', handleMagic)-->
<!--    function handleMagic(e) {-->
<!--        if ($navbar.className.indexOf('in') > 0) {-->
<!--            // CLOSE-->
<!--            $navbar.className = " ";-->
<!--            // wait until animation end.-->
<!--            setTimeout(function () {-->
<!--                // prevent frequently toggle-->
<!--                if ($navbar.className.indexOf('in') < 0) {-->
<!--                    $collapse.style.height = "0px"-->
<!--                }-->
<!--            }, 400)-->

<!--            var pos = $toggle.className.indexOf("is-active");-->
<!--            if (pos > 0) {-->
<!--                $toggle.className = $toggle.className.substr(0, pos);-->
<!--            }-->


<!--        } else {-->
<!--            // OPEN-->
<!--            $collapse.style.height = "auto"-->
<!--            $navbar.className += " in";-->
<!--            $toggle.className += " is-active";-->
<!--        }-->
<!--    }-->
<!--</script>-->


<!-- Main Content -->

<!-- Image to hack wechat -->
<!-- <img src="http://danfergo.github.io/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="{{ site.baseurl }}/{% if page.header-img %}{{ page.header-img }}{% else %}{{ site.header-img }}{% endif %}" width="0" height="0"> -->

<!-- Post Header -->
<!--<style type="text/css">-->
<!--header.intro-header{-->
<!--background-image: url('/img/home-bg.jpg')-->
<!--}-->
<!--</style>-->
<header class="intro-header">
    <div class="center-wrapper">
        <div class="post-heading article-wrapper">
            <div class="tags">
                
                    <a class="tag" href="/tags/#vision"
                       title="vision">vision</a>
                
            </div>
            <h1>GarmNet: Improving Global with Local Perception for Robotic Laundry Folding</h1>
            <h2 class="subheading"></h2>
            <div class="meta">
                
                    <div><span ><a href="/">Daniel Fernandes Gomes</a>, </span><span ><a target="_blank" rel="noopener" href="https://shanluo.github.io/">Shan Luo</a></span><span  style="word-break: keep-all; white-space: nowrap;"> and <a target="_blank" rel="noopener" href="https://scholar.google.pt/citations?user=WLOWIngoRCoC">Luis Filipe Teixeira</a></span></div>
                
                <div style="padding-top:1rem">
                    March 2019
                </div>
                <!--Posted by danfergo on -->
            </div>
        </div>


        <div class="center-wrapper article-wrapper" style="padding: 1rem 0;">
            
            
            
        </div>
    </div>
    <div style="background: #f3f3f3">
        <div class="center-wrapper article-wrapper">
            
            
        </div>
    </div>
    <style>
        .header-btn,
        .header-btn:hover,
        .header-btn:active,
        .header-btn:visited,
        .header-btn:focus {
            border-radius: 3rem;
            background: black;
            border: 2px solid white;
            padding: 0.8rem 1.6rem;
            color: white;
            margin-left: 0.5rem;
            outline: none !important;
            box-shadow: none !important;
        }

        @media (max-width: 767px) {
            .header-btn,
            .header-btn:hover,
            .header-btn:active,
            .header-btn:visited,
            .header-btn:focus {
                background: transparent;
                font-size: 1.5rem;
                display: inline-flex;
                height: 3.5rem;
                width: 3.5rem;
                border-radius: 999px;
                align-items: center;
                justify-content: center;
                border: none;
            }

            .header-btn span {
                display: none;
            }
        }

        .collapsable-blind {
            height: 0;
            overflow: hidden;
            box-sizing: content-box;
            transition: ease all 0.5s;
        }

        .collapsable-blind.open {
            height: auto;
        }

    </style>
    <script type="text/javascript">
        const collapsableBtns = document.querySelectorAll('.collapsable-btn');
        const collapsableBlinds = document.querySelectorAll('.collapsable-blind');

        let openingTimeout = null;

        collapsableBtns.forEach(btn => {
            btn.addEventListener('click', (ev) => {
                const collapsableName = btn.dataset.for;
                const citeThisBlind = document.querySelector(`.collapsable-blind[data-of='${collapsableName}']`);

                clearTimeout(openingTimeout);

                let sameClosing = false;
                collapsableBlinds.forEach(blind => {
                    if (blind.classList.contains('opening') || blind.classList.contains('open')) {
                        if (blind.dataset.of === collapsableName) {
                            sameClosing = true;
                        }
                        const innerContent = blind.querySelector('.inner-container');
                        blind.style.height = innerContent.clientHeight + 'px';
                        blind.classList.remove('opening');
                        blind.classList.remove('open');
                        setTimeout(() => {
                            blind.style.removeProperty('height');
                        })
                    }
                });

                if (!sameClosing) {
                    const innerContent = citeThisBlind.querySelector('.inner-container');
                    citeThisBlind.style.height = innerContent.clientHeight + 'px';
                    citeThisBlind.classList.add('opening');
                    openingTimeout = setTimeout(() => {
                        citeThisBlind.classList.remove('opening');
                        citeThisBlind.classList.add('open');
                        citeThisBlind.style.removeProperty('height');
                    }, 500);
                }
            })
        });
    </script>
</header>

<!-- Post Content -->
<!-- Post Container -->
<div class="center-wrapper post-container">
    <article>
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><span class="init-cap">D</span>eveloping autonomous assistants to help with domestic tasks is a vital topic in robotics research. Among these tasks, garment folding is one of them that is still far from being achieved mainly due to the large number of possible configurations that a crumpled piece of clothing may exhibit. Research has been done on either estimating the pose of the garment as a whole or detecting the landmarks for grasping separately. However, such works constrain the capability of the robots to perceive the states of the garment by limiting the representations for one single task. In this work, we propose a novel end-to-end deep learning model named GarmNet that is able to simultaneously localize the garment and detect landmarks for grasping. The localization of the garment represents the global information for recognising the category of the garment, whereas the detection of landmarks can facilitate sub-sequent grasping actions. We train and evaluate our proposed GarmNet model using the CloPeMa Garment dataset that contains 3,330 images of different garment types in different poses. The experiments show that the inclusion of landmark detection (GarmNet-B) can largely improve the garment localization, with an error rate of 24.7% lower. Approaches as ours are important for robotics applications, as these offer scalable to many classes, memory and processing efficient solutions.</p>
<p>So, why study this problem? The central motivation lies in automating the folding of clothing items, an activity that is present throughout our society and either consumes our personal time or is reflected in the cost of services or products we purchase. Any system we can imagine for automating this task will undoubtedly require a clothing recognition module, which is the focus of this research.<br>Furthermore, when considering clothing recognition in isolation, it becomes evident that this capability is not only useful for folding clothes but also for other systems. For example, intelligent surveillance systems may want to locate a person in a video feed based on their clothing description, or e-commerce recommendation systems may need to provide suggestions based solely on information available in a photograph. By developing a reliable and efficient model for clothing recognition, we aim to contribute to the advancement of these technologies and improve their performance in various applications.</p>
<figure class="fig-caption">
            <img style="width:100%; margin:0;padding: 1%;" src="/garmnet/problem.jpg">
            <figcaption><strong>Figure 1. </strong> Spatial Constraint</figcaption>
          </figure>

<!--
## Background
Traditional Convolutional neural networks (CNNs) can be broken down into two main blocks: Convolutional layers, that have the goal of iteratively and hierarchically extract descriptive features from the raw image; followed by Linear (or fully connected) layers, weigh such features into a final prediction.
When examining previous works on object detection, the first and most influential is the R-CNN series, which inspired the design of our model. The R-CNN series has been improved over three papers. In the first one, the core idea of the model is defined, with the help of traditional machine learning models. In the subsequent papers, new operations and components are introduced, enabling the entire model to be viewed as a convolutional network.
A notable component is the Region Proposal Network (RPN), introduced in the third paper, Faster R-CNN. For each position in the last activation map of a set of convolutional operations, the RPN makes a set of binary predictions (classification and localization) that serve as proposals for the next stage of the model.
Another key component is the Region of Interest (RoI) pooling operation, which extracts and resizes positive proposals made by the RPN to a fixed size. These proposals are then classified in the subsequent stages of the model, and the bounding box parameters are adjusted accordingly. These innovations have greatly contributed to the development of more efficient and accurate object detection models, providing the foundation for our clothing recognition system.
However, the R-CNN model has a significant computational inefficiency due to the classification and regression being performed in two steps.
This problem is addressed in the YOLO model, which uses two outputs. The first output determines a large set of bounding boxes, each associated with a confidence score. The second output calculates a conditional probability for each cell in a predefined grid. If a positive bounding box exists for a cell, the object is considered to belong to the predicted class.
By streamlining the process and performing object detection in a single step, the YOLO model significantly improves computational efficiency while maintaining high accuracy. This approach has influenced the design of our clothing recognition model, allowing it to effectively analyze images and deliver accurate results in a more efficient manner.
Another work, specifically focused on clothing item recognition, is DeepFashion. This model not only uses the image itself but also incorporates meta-information to determine the keypoints of interest (without considering the class) of the clothing item, its class, and additional attributes related to the type of clothing, material, and context of use.
DeepFashion is similar to the R-CNN model in that it has intermediate proposals without considering their specific class. However, in this model, these proposals are used to feed a final component that doesn't classify them but instead determines the class and attributes of the clothing item.
Additionally, this model includes a third branch used to determine global features of the image. By combining these approaches, DeepFashion achieves a more comprehensive understanding of clothing items in images, allowing it to accurately recognize and process various types of garments and their associated attributes.
-->

<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>Given an image containing a clothing item in a semi-crumpled or semi-folded pose, we aim to develop a system capable of classifying and localizing the clothing item with a bounding box. Additionally, the system should be able to detect and classify each associated keypoint of interest on the clothing item, such as the outer lower corner of the right leg or the position between the legs. By tackling this problem, we hope to create a robust solution that can be integrated into various applications, ultimately improving their efficiency and effectiveness in recognizing and processing clothing items.</p>
<p>Our model was initially inspired by the R-CNN architecture. However, after undergoing several modifications, it has also come to resemble other models such as YOLO and DeepFashion. Similar to YOLO, our model performs object detection in a single step. It shares some aspects with DeepFashion by using keypoint predictions to enhance the overall image feature prediction, ultimately improving the classification of the object. Our model also has its unique characteristics, such as incorporating an extra class for background segmentation. We can break down the model into three primary modules: Feature Extraction, and two additional branches. One branch is responsible for determining the class and bounding box of the clothing item, while the other branch focuses on identifying the keypoints of interest. This combination of elements allows our model to efficiently analyze images and deliver accurate results. </p>
<p><strong>The feature extractor</strong> in our model is implemented using a 50-layer ResNet, which has been pre-trained on datasets provided by the ImageNet platform. From this model, we have chosen two output points. The first one produces a 14x14 volume, which we use as input for the keypoint detection module. The second output point generates a vector with 2048 features, which we use as input for the clothing item localization and classification module. The set of features computed between the first and second output points can be interpreted as global features of the image. This allows our model to effectively analyze the overall structure and content of the image, contributing to its accuracy and efficiency in detecting objects and keypoints.</p>
<p><strong>The clothing item localization and classification</strong> module consists of an intermediate dense operation and two outputs. One output performs the bounding box regression for the clothing item, while the other predicts the clothing item’s class, encoded in a one-hot format. The regression output is trained using the mean squared error loss function, while the classification output is trained using the cross-entropy loss function, applied in conjunction with the softmax activation.</p>
<p><strong>The keypoint detector can be interpreted</strong> as applying a small localizer for each spatial position of the feature extractor’s output volume, implemented with convolutional operations. In other words, the intermediate layer has a 3x3 filter and 1024 activation maps, which is equivalent to applying a dense operation with an output vector of 1024 features. Subsequently, we apply the dense operation to each of these feature vectors using a convolutional operation with a 1x1 filter. This approach enables our model to effectively detect keypoints of interest on the clothing items within the image.</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2>
<!-- 
Accuracy is the percentage of correct predictions. When considering accuracy for object localization the IoU is also considered i.e., must be higher than 50%.
mean Average Precision is (related to) the area under the precision-recall curve. The curve is defined by computing the precision and recall each rank level. A confidence value for each proposal is required, to sort the proposals. IoU of 50% is also applied.

Baseline.
-->

<p>We train each branch of the model separately during 40 iterations, batch size of 30 and Adadelta optimizer. The Garment localizer achieves: 100% accuracy on classification and 43% classification+localization. The Landmark detector achieves: 37.8% mAP.</p>

<!-- 
Firstly, we wanted to take advantage of the fact that there is only one class of keypoints of interest per image. In other words, if we can use cross-entropy loss along the depth dimension because, given the softmax activation, these features follow a probabilistic distribution, we can also assume that there is a spatial probabilistic distribution if we apply softmax spatially as well. With this approach, we aimed to reduce the space of possible solutions. However, the mean Average Precision (mAP) dropped from 37.8% to 35.7%. This result indicates that while the proposed method aimed to improve the model's performance, it did not produce the desired outcome. Further experimentation and modifications may be needed to achieve better results in detecting and recognizing clothing items and their associated keypoints.
In a second experiment, we introduced the features of the activation maps from the last layer of the keypoints detector into the module for localization and classification of the clothing items' keypoints. This was achieved by flattening the output volume of the keypoints detector and concatenating the resulting vector with the output vector from the intermediate dense operation of the clothing item localizer. As a consequence, the classification and localization accuracy improved significantly from 43% to 82%. This result demonstrates that incorporating the keypoints detector's activation maps into the localization and classification module can greatly enhance the model's performance in identifying and locating clothing items and their associated keypoints.
-->

<figure class="fig-caption">
            <img style="width:100%; margin:0;padding: 1%;" src="/garmnet/img_spacial_constraint.png">
            <figcaption><strong>Figure 2. </strong> Spatial Constraint</figcaption>
          </figure>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>We have shown a possible way to perform landmark detection together with garment localization, based on a object detection approach. We should notice, though, the fact that for some landmark classes the AP is 0% i.e., no predictions are made. This, together with the fact that many hyper-parameters can be tweaked and the dataset is not very large, hints  to the fact that better results could be obtained. To attempt to tackle more complex problems, different from simple classification and/or regression, it is important to understand each type of module, and the gradient descent dynamics. When implementing custom losses or layers,  the vectorial paradigm is always present. Better tools for the development of such systems can be envisioned, that would contribute to quicker design and implementation iterations. Instead of attempting to extract all the landmarks and garment class+localization in one step, other approaches should be considered. For instance, given an image what’s the best point to grasp? Or, how much folded is the present garment piece?</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">GarmNet: Improving Global with Local Perception for Robotic Laundry Folding, TAROS 2019, <strong><a href="%5Bhttps://arxiv.org/abs/1907.00408%5D">arxiv</a></strong><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">Detecting garment and its landmarks MSc dissertation, 2017-07-18. <strong><a target="_blank" rel="noopener" href="https://hdl.handle.net/10216/107701">UP Open Repository</a></strong><a href="#fnref:2" rev="footnote"> ↩</a></span></li></ol></div></div>
    </article>


    <hr>
    <!--    <ul class="pager">-->
    <!--        -->
    <!--            <li class="previous">-->
    <!--                <a href="/geltip/" data-toggle="tooltip" data-placement="top"-->
    <!--                   title="GelTip: A Finger-shaped Optical Tactile Sensor for Robotic Manipulation">&larr; Previous Post</a>-->
    <!--            </li>-->
    <!--        -->
    <!--        -->
    <!--            <li class="next">-->
    <!--                <a href="/lidaco/" data-toggle="tooltip" data-placement="top"-->
    <!--                   title="Lidaco:  A Modular Wind Lidar Data Converter">Next Post &rarr;</a>-->
    <!--            </li>-->
    <!--        -->
    <!--    </ul>-->

    
        <!-- disqus 评论框 start -->
        <div class="comment">
            <div id="disqus_thread" class="disqus-thread"></div>
        </div>
        <!-- disqus 评论框 end -->
    
</div>
<!--&lt;!&ndash; Side Catalog Container &ndash;&gt;-->
<!---->


    <!-- disqus start -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES * * */
        /**
         *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
         *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */

        var disqus_shortname = "danfergo";
        var disqus_identifier = "http://danfergo.github.io/garmnet/";
        var disqus_url = "http://danfergo.github.io/garmnet/";

        (function () {
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <!-- disqus end -->




<!-- Footer -->
<!-- Footer -->
<footer>
    <style>
        /* Tomorrow Night Theme */
        /* https://jmblog.github.io/color-themes-for-google-code-highlightjs */
        /* Original theme - https://github.com/chriskempson/tomorrow-theme */
        /* https://jmblog.github.io/color-themes-for-google-code-highlightjs */
        .tomorrow-comment,
        .post-container pre .comment,
        .post-container pre .title {
            color: #969896;
        }

        .tomorrow-red,
        .post-container pre .variable,
        .post-container pre .attribute,
        .post-container pre .tag,
        .post-container pre .regexp,
        .post-container pre .ruby .constant,
        .post-container pre .xml .tag .title,
        .post-container pre .xml .pi,
        .post-container pre .xml .doctype,
        .post-container pre .html .doctype,
        .post-container pre .css .id,
        .post-container pre .css .class,
        .post-container pre .css .pseudo {
            color: #cc6666;
        }

        .tomorrow-orange,
        .post-container pre .number,
        .post-container pre .preprocessor,
        .post-container pre .built_in,
        .post-container pre .literal,
        .post-container pre .params,
        .post-container pre .constant {
            color: #de935f;
        }

        .tomorrow-yellow,
        .post-container pre .class,
        .post-container pre .ruby .class .title,
        .post-container pre .css .rules .attribute {
            color: #f0c674;
        }

        .tomorrow-green,
        .post-container pre .string,
        .post-container pre .value,
        .post-container pre .inheritance,
        .post-container pre .header,
        .post-container pre .ruby .symbol,
        .post-container pre .xml .cdata {
            color: #b5bd68;
        }

        .tomorrow-aqua,
        .post-container pre .css .hexcolor {
            color: #8abeb7;
        }

        .tomorrow-blue,
        .post-container pre .function,
        .post-container pre .python .decorator,
        .post-container pre .python .title,
        .post-container pre .ruby .function .title,
        .post-container pre .ruby .title .keyword,
        .post-container pre .perl .sub,
        .post-container pre .javascript .title,
        .post-container pre .coffeescript .title {
            color: #81a2be;
        }

        .tomorrow-purple,
        .post-container pre .keyword,
        .post-container pre .javascript .function {
            color: #b294bb;
        }

        .post-container .highlight,
        .post-container pre {
            display: block;
            /*background: #1d1f21;*/
            background: #1a1a1a;
            color: #c5c8c6;
            font-family: Menlo, Monaco, Consolas, monospace;
            line-height: 1.5;
            border: 1px solid #ccc;
            padding: 10px;
        }
    </style>
    <div class="short-about">
        
            <a href="/about/" title="More about me"> <img
                        src="/img/avatar.jpg"/></a>
        
    </div>
    <div class="short-about">

        
            <p>Daniel Fernandes Gomes or @danfergo is a passionate Researcher and Engineer on Intelligence. 
                
            </p>
        
            <p>Research associate at Kings College London and PhD student at the University of Liverpool.
                
                    <a href="/about/" title="More about me"> More. </a>
                
            </p>
        
    </div>
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="text-center">
            <ul class="list-inline">
    
        <li>
            <a href="/atom.xml">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>
    

    
        <li>
            <a target="_blank" href="https://github.com/danfergo">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>
    
        <li>
            <a target="_blank" href="https://scholar.google.com/citations?user=qFc_b5MAAAAJ">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>
    
        <li>
            <a target="_blank" href="https://youtube.com/danfergo">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-youtube-play fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>
    
        <li>
            <a target="_blank" href="https://linkedin.com/in/danfergo">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>
    
</ul>
        </div>
        <p class="copyright text-muted">
            <a href="https://github.com/Kaijun/hexo-theme-huxblog" target="_blank">Hux</a> for <a
                    href="https://hexo.io/" target="_blank">Hexo</a><br>
            <!--                    &copy; Danfergo Research 2023-->
        </p>
    </div>
</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>



<script src="/js/carousel.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.js"></script>



<!-- async load function -->
<script>
    function async(u,
                   c) {
        var d = document,
            t = 'script',

            o = d.createElement(t),

            s = d.getElementsByTagName(t)[0];
        o.src = u;
        if (c) {
            o.addEventListener('load',
                function (e) {
                    c(null,
                        e);
                },
                false);
        }
        s.parentNode.insertBefore(o,
            s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,

     From V1.6,
 There is no need for Highlight.js,

     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js",
 function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<!--<script>-->
<!--// only load tagcloud.js in tag.html-->
<!--if($('#tag_cloud').length !== 0){-->
<!--async("http://danfergo.github.io/js/jquery.tagcloud.js",
function(){-->
<!--$.fn.tagcloud.defaults = {-->
<!--//size: {start: 1,
 end: 1,
 unit: 'em'},
-->
<!--color: {start: '#bbbbee',
 end: '#0085a1'},
-->
<!--};-->
<!--$('#tag_cloud a').tagcloud();-->
<!--})-->
<!--}-->
<!--</script>-->

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js",
        function () {
            var $nav = document.querySelector("nav");
            if ($nav) FastClick.attach($nav);
        })
</script>


<!-- Google Analytics -->


    <script>
        // Originial
        (function (i,
                   s,
                   o,
                   g,
                   r,
                   a,
                   m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            },
                i[r].l = 1 * new Date();
            a = s.createElement(o),

                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a,
                m)
        })(window,
            document,
            'script',
            '//www.google-analytics.com/analytics.js',
            'ga');

        ga('create',
            'UA-85798112-1',
            'auto');
        ga('send',
            'pageview');

        $('body').on('click', 'a', function (e) {
            var _ = $(this);
            if (_[0].pathname.indexOf('.') > -1 && _[0].hostname === window.location.hostname) {

                if (_.data('prevented') === 1) {
                    _.removeData("prevented");
                    return true;
                }

                e.preventDefault();
                _.data('prevented', 1);
                window.__gacb = function () {
                    _[0].click();
                };
                console.log('Download:', _[0].pathname);
                ga('send', 'pageview', {
                    'page': _[0].pathname,
                    'hitCallback': function () {
                        window.__gacb();
                    }
                })
                return false;
            }
        });


    </script>




<!-- Side Catalog -->




</body>
</html>
